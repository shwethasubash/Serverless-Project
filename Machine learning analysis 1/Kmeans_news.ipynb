{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author : Yashesh Savani\n",
    "# Date Created: 26th July, 2020\n",
    "# Serverless Project: Machine Learning Analysis 1\n",
    "# Reference:\n",
    "# “K-Means Clustering with scikit-learn,” Jonathan Soma makes things. [Online]. \n",
    "# Available: http://jonathansoma.com/lede/algorithms-2017/classes/clustering/k-means-clustering-with-scikit-learn/. [Accessed: 26-Jul-2020]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "import bs4\n",
    "import re\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SRC_BUCKET\"] = \"newsreutersfiles\"\n",
    "os.environ[\"DEST_MODEL_BUCKET\"] = \"kmeansmodels\"\n",
    "os.environ[\"NUM_CLUSTERS\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_bucket = os.getenv(\"SRC_BUCKET\")\n",
    "dest_model_bucket = os.getenv(\"DEST_MODEL_BUCKET\")\n",
    "num_clusters = int(os.getenv(\"NUM_CLUSTERS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_FILE_NAME = \"titles.txt\"\n",
    "KMEANS_MODEL_PKL = \"kmeans_model.pkl\"\n",
    "FEATURES_PKL = \"feature.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Titles from the files\n",
    "def extract_titles_from_files():\n",
    "    client_storage = storage.Client()\n",
    "    # Get object of the bucket where files will be uploaded\n",
    "    src_bucket_files_blob = client_storage.list_blobs(src_bucket)    # Get source bucket object\n",
    "    \n",
    "    # Get source bucket object\n",
    "    src_bucket_object = client_storage.get_bucket(src_bucket)\n",
    "    \n",
    "    # Loop through all the file objects got from GCP storage\n",
    "    for file in src_bucket_files_blob:\n",
    "        file_blob = src_bucket_object.get_blob(file.name)\n",
    "        file_data = file_blob.download_as_string()\n",
    "        soup = bs4.BeautifulSoup(file_data, \"html.parser\")\n",
    "        titles_in_file = soup.find_all(\"title\")\n",
    "        with open(TITLE_FILE_NAME, \"a\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            for title in titles_in_file:\n",
    "                title_text = re.sub(r\"<[-.'/()\\s\\w]*[<>)]\", \"\", title.text).strip()\n",
    "                f.write(title_text.lower() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_titles_from_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model of word vector got from CountVectorizer\n",
    "def create_model(FILE_NAME):\n",
    "    with open(FILE_NAME, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        file_data = f.read().split(\"\\n\")\n",
    "\n",
    "    title_word_vector = CountVectorizer(stop_words='english')\n",
    "    word_matrix = title_word_vector.fit_transform(file_data)\n",
    "    df = pd.DataFrame(word_matrix.toarray(), columns=title_word_vector.get_feature_names())\n",
    "    pickle.dump(title_word_vector.vocabulary_, open(FEATURES_PKL, \"wb\"))\n",
    "    return word_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature.pkl from titles gathered from training data.\n",
    "word_matrix = create_model(TITLE_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using training titles data\n",
    "def kmeans_clustering(word_matrix, FILE_NAME):\n",
    "    \n",
    "    with open(FILE_NAME, encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        file_data = f.read().split(\"\\n\")\n",
    "    kmeans_m = KMeans(n_clusters=num_clusters)\n",
    "    kmeans_m.fit(word_matrix)\n",
    "    labels = kmeans_m.labels_\n",
    "    df = pd.DataFrame()\n",
    "    df[\"title\"] = file_data\n",
    "    df[\"cluster_number\"] = labels\n",
    "    \n",
    "    # Initialise storage object\n",
    "    client_storage = storage.Client()\n",
    "    # Get destination bucket object\n",
    "    dest_bucket_object = client_storage.get_bucket(dest_model_bucket)\n",
    "    # Upload kmeansmodel.pkl to GCP storage \n",
    "    dest_bucket_blob = dest_bucket_object.blob(\"Title_cluster.csv\") \n",
    "    df.to_csv(\"Title_cluster.csv\", index=False)\n",
    "    dest_bucket_blob.upload_from_filename(\"Title_cluster.csv\")\n",
    "    pickle.dump(kmeans_m, open(KMEANS_MODEL_PKL, \"wb\"))\n",
    "    return kmeans_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_m = kmeans_clustering(word_matrix,TITLE_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload trained model to bucket for further use\n",
    "def upload_models_to_bucket():\n",
    "    \n",
    "    # Initialise storage object\n",
    "    client_storage = storage.Client()\n",
    "    # Get destination bucket object\n",
    "    dest_bucket_object = client_storage.get_bucket(dest_model_bucket)\n",
    "    \n",
    "    # Upload kmeansmodel.pkl to GCP storage \n",
    "    dest_bucket_blob = dest_bucket_object.blob(KMEANS_MODEL_PKL) \n",
    "    with open(KMEANS_MODEL_PKL, \"rb\") as f:\n",
    "        dest_bucket_blob.upload_from_file(f)\n",
    "\n",
    "    # Upload feature.pkl to GCP storage \n",
    "    dest_bucket_blob = dest_bucket_object.blob(FEATURES_PKL) \n",
    "    with open(FEATURES_PKL, \"rb\") as f:\n",
    "        dest_bucket_blob.upload_from_file(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_models_to_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_kmeans(MODEL_PATH, test_sentence, FEATURE_PATH):\n",
    " \n",
    "    kmeans_model = pickle.load(open(MODEL_PATH, \"rb\"))\n",
    "    vocab = pickle.load(open(FEATURE_PATH, \"rb\"))\n",
    "    title_word_vector = CountVectorizer(vocabulary=vocab)\n",
    "    matrix = title_word_vector.transform([test_sentence])\n",
    "    \n",
    "    pred = kmeans_model.predict(matrix)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "test_kmeans(\"kmeans_model.pkl\", \"BAHIA COCOA REVIEW\",\"feature.pkl\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m52",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m52"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}